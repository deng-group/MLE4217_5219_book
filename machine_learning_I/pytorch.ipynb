{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical: PyTorch\n",
    "In this practical, we will learn how to use PyTorch for some machine learning tasks.\n",
    "\n",
    "[PyTorch](https://pytorch.org) is a popular open-source machine learning library, particularly known for its flexibility and Pythonic feel. It's widely used for building and training neural networks. Let's cover the fundamental building blocks you'll encounter. It is widely used in academia and industry for deep learning applications: like computer vision, natural language processing, and reinforcement learning.\n",
    "\n",
    "## Features\n",
    "- Tensors: PyTorch provides a powerful tensor library that allows for efficient computation on multi-dimensional arrays. Tensors are similar to NumPy arrays but can be used on GPUs for faster computation.\n",
    "- Autograd: PyTorch has a built-in automatic differentiation engine that allows for easy computation of gradients. This is particularly useful for training neural networks using backpropagation.\n",
    "- Dynamic computation graph: PyTorch uses a dynamic computation graph, which means that the graph is built on-the-fly as operations are performed. This allows for more flexibility in building complex models and makes debugging easier.\n",
    "- Neural networks: PyTorch provides a high-level API for building and training neural networks. It includes pre-defined layers, loss functions, and optimizers that make it easy to build complex models.\n",
    "- GPU support: PyTorch can easily switch between CPU and GPU computation, making it easy to take advantage of the speedup provided by GPUs.\n",
    "- Community: PyTorch has a large and active community, which means that there are many resources available for learning and troubleshooting. There are also many pre-trained models and libraries available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "!pip3 install torch torchvision torchaudio torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors: The Core Data Structure\n",
    "Tensors are the central data structure in PyTorch, similar to NumPy arrays but with added capabilities, notably the ability to run on GPUs for accelerated computation and automatic differentiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from list:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Tensor from NumPy array:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Random Tensor:\n",
      " tensor([[0.2063, 0.8167, 0.6958],\n",
      "        [0.4978, 0.8836, 0.5970]]) \n",
      "\n",
      "Ones Tensor:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n",
      "Apple Silicon GPU (MPS) is available!\n",
      "Device tensor_mps is stored on: mps:0\n",
      "Addition:\n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "\n",
      "Element-wise Multiplication:\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "\n",
      "Matrix Multiplication:\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "\n",
      "Tensor converted back to NumPy:\n",
      "[[ 6.  8.]\n",
      " [10. 12.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Creating Tensors ---\n",
    "\n",
    "# From a list\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f\"Tensor from list:\\n{x_data}\\n\")\n",
    "\n",
    "# From a NumPy array (shares memory by default!)\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"Tensor from NumPy array:\\n{x_np}\\n\")\n",
    "\n",
    "# Creating tensors with specific shapes and values\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape) # Random values between 0 and 1\n",
    "ones_tensor = torch.ones(shape)   # All ones\n",
    "zeros_tensor = torch.zeros(shape)  # All zeros\n",
    "print(f\"Random Tensor:\\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor:\\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor:\\n {zeros_tensor}\\n\")\n",
    "\n",
    "# --- Tensor Attributes ---\n",
    "tensor = torch.rand(3, 4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\") # Default is CPU\n",
    "\n",
    "# --- Moving Tensors to GPU (if available) ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU) is available!\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    # Move tensor to GPU\n",
    "    tensor_gpu = tensor.to(device)\n",
    "    print(f\"Device tensor_gpu is stored on: {tensor_gpu.device}\")\n",
    "    # Operations on tensor_gpu will run on the GPU\n",
    "\n",
    "    # Move back to CPU (e.g., for use with NumPy)\n",
    "    tensor_cpu = tensor_gpu.to(\"cpu\")\n",
    "    # NOTE: NumPy cannot handle GPU tensors directly.\n",
    "\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Apple Silicon GPU (MPS) is available!\")\n",
    "    device = torch.device(\"mps\")\n",
    "    # Move tensor to MPS\n",
    "    tensor_mps = tensor.to(device)\n",
    "    print(f\"Device tensor_mps is stored on: {tensor_mps.device}\")\n",
    "    # Operations on tensor_mps will run on the Apple Silicon GPU\n",
    "\n",
    "    # Move back to CPU (e.g., for use with NumPy)\n",
    "    tensor_cpu = tensor_mps.to(\"cpu\")\n",
    "    # NOTE: NumPy cannot handle MPS tensors directly.\n",
    "else:\n",
    "    print(\"CUDA or Apple Silicon (GPU) not available, using CPU.\")\n",
    "    device = torch.device(\"cpu\") # Use CPU if GPU not available\n",
    "\n",
    "# --- Basic Operations ---\n",
    "# Similar syntax to NumPy\n",
    "tensor_a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "tensor_b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "# Element-wise addition\n",
    "sum_tensor = tensor_a + tensor_b\n",
    "# or torch.add(tensor_a, tensor_b)\n",
    "print(f\"Addition:\\n{sum_tensor}\\n\")\n",
    "\n",
    "# Element-wise multiplication\n",
    "mul_tensor = tensor_a * tensor_b\n",
    "# or torch.mul(tensor_a, tensor_b)\n",
    "print(f\"Element-wise Multiplication:\\n{mul_tensor}\\n\")\n",
    "\n",
    "# Matrix multiplication\n",
    "matmul_tensor = tensor_a @ tensor_b\n",
    "# or torch.matmul(tensor_a, tensor_b)\n",
    "print(f\"Matrix Multiplication:\\n{matmul_tensor}\\n\")\n",
    "\n",
    "# --- NumPy Bridge ---\n",
    "# Tensor to NumPy array\n",
    "numpy_array_again = sum_tensor.numpy() # Only works for CPU tensors\n",
    "print(f\"Tensor converted back to NumPy:\\n{numpy_array_again}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd: Automatic Differentiation\n",
    "This is one of PyTorch's most powerful features. If a tensor is created with `requires_grad=True`, PyTorch tracks all operations performed on it. When you finish your computation (e.g., calculating a loss), you can automatically compute gradients using `.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "\n",
      "y = x + 2:\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "y.grad_fn: <AddBackward0 object at 0x11fd3be20>\n",
      "\n",
      "z = y*y*3:\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "\n",
      "out = z.mean(): 27.0\n",
      "\n",
      "Gradient of out w.r.t. x (d(out)/dx):\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "\n",
      "y_no_grad.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# Create tensors that require gradient tracking\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(f\"x:\\n{x}\\n\")\n",
    "\n",
    "# Perform an operation\n",
    "y = x + 2\n",
    "print(f\"y = x + 2:\\n{y}\\n\")\n",
    "# y was created as a result of an operation involving x, so it has a grad_fn\n",
    "print(f\"y.grad_fn: {y.grad_fn}\\n\")\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean() # Calculate a scalar value\n",
    "print(f\"z = y*y*3:\\n{z}\\n\")\n",
    "print(f\"out = z.mean(): {out}\\n\")\n",
    "\n",
    "# --- Compute gradients ---\n",
    "# out is a scalar, so we can call backward() directly\n",
    "out.backward()\n",
    "\n",
    "# Gradients are accumulated in the .grad attribute of the tensors\n",
    "# d(out)/dx is computed\n",
    "print(f\"Gradient of out w.r.t. x (d(out)/dx):\\n{x.grad}\\n\")\n",
    "\n",
    "# Let's verify manually for one element:\n",
    "# out = (1/4) * sum(z) = (1/4) * sum(3 * (x+2)^2)\n",
    "# d(out)/dx_ij = (1/4) * d/dx_ij [ 3 * (x_ij+2)^2 ]\n",
    "#             = (1/4) * 3 * 2 * (x_ij+2) * 1\n",
    "#             = (3/2) * (x_ij+2)\n",
    "# Since x starts as ones(2,2), x_ij = 1.\n",
    "# d(out)/dx_ij = (3/2) * (1+2) = (3/2) * 3 = 4.5\n",
    "# The output tensor x.grad should be [[4.5, 4.5], [4.5, 4.5]]\n",
    "\n",
    "# Turn off gradient tracking when not needed (e.g., during evaluation)\n",
    "with torch.no_grad():\n",
    "    y_no_grad = x + 2\n",
    "    print(f\"y_no_grad.requires_grad: {y_no_grad.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Module`: Building Neural Networks\n",
    "\n",
    "PyTorch provides the torch.nn package to build neural networks. You typically define your network as a class inheriting from nn.Module.\n",
    "\n",
    "- `__init__(self)`: Define the layers of your network here (e.g., linear layers, activation functions).\n",
    "\n",
    "- `forward(self, x)`: Define how input x flows through the layers defined in __init__ to produce the output. Autograd automatically builds the computation graph based on this forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Model Structure:\n",
      "SimpleMLP(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model Parameters:\n",
      "network.0.weight torch.Size([64, 10])\n",
      "network.0.bias torch.Size([64])\n",
      "network.2.weight torch.Size([32, 64])\n",
      "network.2.bias torch.Size([32])\n",
      "network.4.weight torch.Size([1, 32])\n",
      "network.4.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Example: A simple Linear Regression Model (y = Wx + b)\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # Define the layer(s)\n",
    "        self.linear = nn.Linear(input_dim, output_dim) # One linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "# Example: A Multi-Layer Perceptron (like used in the demos)\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size=1, hidden_layers=[64, 32]):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        # Dynamically create hidden layers based on the list\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(nn.ReLU()) # Add ReLU activation after each hidden layer\n",
    "            current_size = hidden_size\n",
    "        # Add the final output layer\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        # Use nn.Sequential to easily chain the layers\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the sequential network\n",
    "        return self.network(x)\n",
    "\n",
    "# Instantiate the MLP model\n",
    "input_features = 10 # Example input size\n",
    "output_value = 1    # Example output size (for regression)\n",
    "model_mlp = SimpleMLP(input_features, output_value)\n",
    "print(\"MLP Model Structure:\")\n",
    "print(model_mlp)\n",
    "\n",
    "# You can easily inspect model parameters\n",
    "print(\"\\nModel Parameters:\")\n",
    "for name, param in model_mlp.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "A loss function measures how far the model's output is from the target value. PyTorch provides many common loss functions in torch.nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example MSE Loss: 2.7126641273498535\n",
      "Example BCE Loss: 0.7222577929496765\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Example Loss Functions\n",
    "criterion_mse = nn.MSELoss() # Mean Squared Error: For regression\n",
    "criterion_bce = nn.BCELoss() # Binary Cross Entropy: For binary classification (with Sigmoid output)\n",
    "criterion_ce = nn.CrossEntropyLoss() # Cross Entropy Loss: For multi-class classification (expects raw logits)\n",
    "\n",
    "# Example usage (assuming model_output and target are tensors)\n",
    "model_output_reg = torch.randn(10, 1, requires_grad=True) # Example regression output\n",
    "target_reg = torch.randn(10, 1)\n",
    "loss_mse = criterion_mse(model_output_reg, target_reg)\n",
    "print(f\"Example MSE Loss: {loss_mse.item()}\") # .item() gets scalar value\n",
    "\n",
    "model_output_cls = torch.sigmoid(torch.randn(10, 1, requires_grad=True)) # Example classification output (post-sigmoid)\n",
    "target_cls = torch.randint(0, 2, (10, 1)).float() # Example binary targets (0 or 1)\n",
    "loss_bce = criterion_bce(model_output_cls, target_cls)\n",
    "print(f\"Example BCE Loss: {loss_bce.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "An optimizer implements an algorithm to update the model's parameters (weights and biases) based on the computed gradients, aiming to minimize the loss function. Common optimizers are found in `torch.optim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Use the MLP model we defined earlier\n",
    "model_to_train = SimpleMLP(10, 1)\n",
    "\n",
    "# --- Choose an Optimizer ---\n",
    "# Adam is a popular and often effective choice\n",
    "optimizer_adam = optim.Adam(model_to_train.parameters(), lr=0.001) # lr is the learning rate\n",
    "\n",
    "# Stochastic Gradient Descent (SGD) is another common one\n",
    "# optimizer_sgd = optim.SGD(model_to_train.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# --- Optimizer Steps (within a training loop) ---\n",
    "# 1. Zero the gradients before calculating loss for a new batch\n",
    "#    optimizer_adam.zero_grad()\n",
    "#\n",
    "# 2. Calculate the loss\n",
    "#    loss = criterion(outputs, targets)\n",
    "#\n",
    "# 3. Compute gradients w.r.t. parameters\n",
    "#    loss.backward()\n",
    "#\n",
    "# 4. Update the model parameters based on gradients\n",
    "#    optimizer_adam.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders\n",
    "\n",
    "`torch.utils.data.Dataset` and `torch.utils.data.DataLoader` provide convenient ways to handle data, batching, shuffling, and parallel loading.\n",
    "\n",
    "*   `Dataset`: Stores the samples and their corresponding labels. You can create custom datasets or use built-in ones. `TensorDataset` is useful when your data already exists as tensors.\n",
    "*   `DataLoader`: Wraps an iterable around the `Dataset` to enable easy access to batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample features shape: torch.Size([10])\n",
      "First sample label shape: torch.Size([1])\n",
      "\n",
      "Iterating through DataLoader:\n",
      "Batch 1:\n",
      "  Features shape: torch.Size([16, 10])\n",
      "  Labels shape: torch.Size([16, 1])\n",
      "Batch 2:\n",
      "  Features shape: torch.Size([16, 10])\n",
      "  Labels shape: torch.Size([16, 1])\n",
      "Batch 3:\n",
      "  Features shape: torch.Size([16, 10])\n",
      "  Labels shape: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Example data (already tensors)\n",
    "features = torch.randn(100, 10) # 100 samples, 10 features each\n",
    "labels = torch.randn(100, 1)   # 100 corresponding labels\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(features, labels)\n",
    "\n",
    "# Access a single sample\n",
    "first_sample_features, first_sample_label = dataset[0]\n",
    "print(f\"First sample features shape: {first_sample_features.shape}\")\n",
    "print(f\"First sample label shape: {first_sample_label.shape}\")\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 16\n",
    "# shuffle=True is important for training to mix data between epochs\n",
    "# num_workers > 0 can speed up loading using parallel processes (use cautiously in notebooks)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Iterate over the DataLoader to get batches\n",
    "print(\"\\nIterating through DataLoader:\")\n",
    "for batch_idx, (batch_features, batch_labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1}:\")\n",
    "    print(f\"  Features shape: {batch_features.shape}\") # Should be [batch_size, 10]\n",
    "    print(f\"  Labels shape: {batch_labels.shape}\")     # Should be [batch_size, 1]\n",
    "    # --- Inside the training loop, you'd process this batch ---\n",
    "    # model(batch_features) -> calculate loss -> loss.backward() -> optimizer.step()\n",
    "    if batch_idx >= 2: # Stop after showing a few batches\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic Training Loop Structure\n",
    "\n",
    "Putting it all together, a typical PyTorch training loop looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Dummy Training Loop:\n",
      "Epoch [1/10], Loss: 14.0250\n",
      "Epoch [2/10], Loss: 13.7025\n",
      "Epoch [3/10], Loss: 13.4990\n",
      "Epoch [4/10], Loss: 13.3793\n",
      "Epoch [5/10], Loss: 13.1886\n",
      "Epoch [6/10], Loss: 13.0974\n",
      "Epoch [7/10], Loss: 12.9531\n",
      "Epoch [8/10], Loss: 12.8884\n",
      "Epoch [9/10], Loss: 12.7939\n",
      "Epoch [10/10], Loss: 12.7382\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# --- Setup (Assume model, criterion, optimizer, train_loader are defined) ---\n",
    "# model = YourModel(...)\n",
    "# criterion = YourLossFunction()\n",
    "# optimizer = YourOptimizer(model.parameters(), lr=...)\n",
    "# train_loader = DataLoader(your_train_dataset, ...)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device) # Move model to the appropriate device\n",
    "\n",
    "# Let's create dummy versions for illustration:\n",
    "model = LinearRegression(5, 1).to(device) # Simple model: 5 features -> 1 output\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "dummy_features = torch.randn(100, 5)\n",
    "dummy_labels = torch.randn(100, 1) * 3 + 2 # y = 3x + 2 + noise approx\n",
    "dummy_dataset = TensorDataset(dummy_features, dummy_labels)\n",
    "train_loader = DataLoader(dummy_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 10\n",
    "print(\"\\nStarting Dummy Training Loop:\")\n",
    "model.train() # Set the model to training mode (important for layers like dropout, batchnorm)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Move data to the same device as the model\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 1. Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2. Forward pass: compute predicted outputs\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 3. Calculate the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # 4. Backward pass: compute gradient of the loss w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for reporting\n",
    "        running_loss += loss.item() * inputs.size(0) # loss.item() gets scalar loss\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# After training, you typically evaluate on a separate test set\n",
    "# model.eval() # Set model to evaluation mode\n",
    "# with torch.no_grad(): # Disable gradient calculation for evaluation\n",
    "#    # ... loop through test_loader and calculate metrics ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
